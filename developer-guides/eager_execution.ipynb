{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square: [[4.]]\n"
     ]
    }
   ],
   "source": [
    "x = [[2.0]]\n",
    "square = tf.matmul(x, x)\n",
    "print(\"square: {}\".format(square))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n",
      "[[ 2  6]\n",
      " [12 20]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1, 2], [3, 4]])\n",
    "b = a + 1\n",
    "print(a)\n",
    "print(b)\n",
    "c = np.multiply(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FizzBuzz\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "Fizz\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def fizzbuzz(max_num):\n",
    "    counter = tf.constant(0)\n",
    "    for num in range(max_num):\n",
    "        num = tf.constant(num)\n",
    "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
    "            print('FizzBuzz')\n",
    "        elif int(num % 3) == 0:\n",
    "            print('Fizz')\n",
    "        elif int(num % 5) == 0:\n",
    "            print('Buzz')\n",
    "        else:\n",
    "            print(num)\n",
    "        counter += 1\n",
    "    return counter\n",
    "counter = fizzbuzz(5)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MySimpleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_units):\n",
    "        self.output_units = output_units\n",
    "    \n",
    "    def build(self):\n",
    "        self.kernel = self.add_variable(\n",
    "      \"kernel\", [input.shape[-1], self.output_units])\n",
    "    \n",
    "    def call(self, input):\n",
    "        return tf.matmul(input, self.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNISTModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units = 10)\n",
    "        self.dense2 = tf.keras.layers.Dense(units = 10)\n",
    "        \n",
    "    def call(self, input):\n",
    "        result = self.dense1(input)\n",
    "        result = self.dense2(result)\n",
    "        result = self.dense2(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=72, shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "w = tfe.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = w * w\n",
    "grad = tape.gradient(loss, [w])\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 70.052\n",
      "Loss at step 000: 67.285\n",
      "Loss at step 020: 30.252\n",
      "Loss at step 040: 13.902\n",
      "Loss at step 060: 6.681\n",
      "Loss at step 080: 3.491\n",
      "Loss at step 100: 2.082\n",
      "Loss at step 120: 1.459\n",
      "Loss at step 140: 1.183\n",
      "Loss at step 160: 1.061\n",
      "Loss at step 180: 1.007\n",
      "Final loss: 0.984\n",
      "W = 3.06354093552, B = 2.1071972847\n"
     ]
    }
   ],
   "source": [
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "def prediction(input, weight, bias):\n",
    "    return input * weight + bias\n",
    "\n",
    "# A loss function using mean-squared error\n",
    "def loss(weights, biases):\n",
    "    error = prediction(training_inputs, weights, biases) - training_outputs\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Return the derivative of loss with respect to weight and bias\n",
    "def grad(weights, biases):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(weights, biases)\n",
    "    return tape.gradient(loss_value, [weights, biases])\n",
    "\n",
    "train_steps = 200\n",
    "learning_rate = 0.01\n",
    "# Start with arbitrary values for W and B on the same batch of data\n",
    "W = tfe.Variable(5.)\n",
    "B = tfe.Variable(10.)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(W, B)))\n",
    "\n",
    "for i in range(train_steps):\n",
    "    dW, dB = grad(W, B)\n",
    "    W.assign_sub(dW * learning_rate)\n",
    "    B.assign_sub(dB * learning_rate)\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(W, B)))\n",
    "print(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for MNIST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_train, dataset_test = tf.keras.datasets.mnist.load_data()\n",
    "mnist_x, mnist_y = dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_x = tf.data.Dataset.from_tensor_slices(mnist_x)\n",
    "mnist_y = tf.data.Dataset.from_tensor_slices(mnist_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mnist import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(model, x, y):\n",
    "    prediction = model(x)\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(model, inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, target)\n",
    "    return tape.gradient(loss_value, model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf Variables and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(tf.keras.Model, self).__init__()\n",
    "        self.W = tfe.Variable(5., name=\"weight\")\n",
    "        self.b = tfe.Variable(10., name=\"bias\")\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        return inputs * self.W + self.b\n",
    "    \n",
    "num_examples = 2000\n",
    "training_inputs = tf.random_normal([num_examples])\n",
    "noise = tf.random_normal([num_examples])\n",
    "training_outputs = training_inputs * 3 + 2 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(model, inputs, y):\n",
    "    target = model.predict(inputs)\n",
    "    error = tf.reduce_mean(tf.square(target - y))\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad(model, inputs, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, y)\n",
    "    return tape.gradient(loss_value, [model.W, model.b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LinearModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init loss: 67.8505935669\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "print(\"init loss: {}\".format(loss(model, training_inputs, training_outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss is 65.2383117676\n",
      "step 20, loss is 29.9568004608\n",
      "step 40, loss is 14.0671339035\n",
      "step 60, loss is 6.90828132629\n",
      "step 80, loss is 3.68180131912\n",
      "step 100, loss is 2.22712731361\n",
      "step 120, loss is 1.57106149197\n",
      "step 140, loss is 1.27507555485\n",
      "step 160, loss is 1.14149880409\n",
      "step 180, loss is 1.0811984539\n",
      "step 200, loss is 1.05396926403\n",
      "step 220, loss is 1.04166996479\n",
      "step 240, loss is 1.03611302376\n",
      "step 260, loss is 1.03360176086\n",
      "step 280, loss is 1.0324665308\n",
      "step 300, loss is 1.03195333481\n",
      "step 320, loss is 1.03172123432\n",
      "step 340, loss is 1.03161621094\n",
      "step 360, loss is 1.03156864643\n",
      "step 380, loss is 1.03154695034\n",
      "step 400, loss is 1.03153729439\n",
      "step 420, loss is 1.03153300285\n",
      "step 440, loss is 1.03153073788\n",
      "step 460, loss is 1.03153014183\n",
      "step 480, loss is 1.03152966499\n",
      "step 500, loss is 1.03152954578\n",
      "step 520, loss is 1.03152954578\n",
      "step 540, loss is 1.03152942657\n",
      "step 560, loss is 1.03152930737\n",
      "step 580, loss is 1.03152906895\n",
      "step 600, loss is 1.03152918816\n",
      "step 620, loss is 1.03152930737\n",
      "step 640, loss is 1.03152930737\n",
      "step 660, loss is 1.03152954578\n",
      "step 680, loss is 1.03152930737\n",
      "step 700, loss is 1.03152930737\n",
      "step 720, loss is 1.03152930737\n",
      "step 740, loss is 1.03152930737\n",
      "step 760, loss is 1.03152930737\n",
      "step 780, loss is 1.03152930737\n",
      "step 800, loss is 1.03152930737\n",
      "step 820, loss is 1.03152930737\n",
      "step 840, loss is 1.03152930737\n",
      "step 860, loss is 1.03152930737\n",
      "step 880, loss is 1.03152930737\n",
      "step 900, loss is 1.03152930737\n",
      "step 920, loss is 1.03152930737\n",
      "step 940, loss is 1.03152930737\n",
      "step 960, loss is 1.03152930737\n",
      "step 980, loss is 1.03152930737\n",
      "step 1000, loss is 1.03152930737\n",
      "step 1020, loss is 1.03152930737\n",
      "step 1040, loss is 1.03152930737\n",
      "step 1060, loss is 1.03152930737\n",
      "step 1080, loss is 1.03152930737\n",
      "step 1100, loss is 1.03152930737\n",
      "step 1120, loss is 1.03152930737\n",
      "step 1140, loss is 1.03152930737\n",
      "step 1160, loss is 1.03152930737\n",
      "step 1180, loss is 1.03152930737\n",
      "step 1200, loss is 1.03152930737\n",
      "step 1220, loss is 1.03152930737\n",
      "step 1240, loss is 1.03152930737\n",
      "step 1260, loss is 1.03152930737\n",
      "step 1280, loss is 1.03152930737\n",
      "step 1300, loss is 1.03152930737\n",
      "step 1320, loss is 1.03152930737\n",
      "step 1340, loss is 1.03152930737\n",
      "step 1360, loss is 1.03152930737\n",
      "step 1380, loss is 1.03152930737\n",
      "step 1400, loss is 1.03152930737\n",
      "step 1420, loss is 1.03152930737\n",
      "step 1440, loss is 1.03152930737\n",
      "step 1460, loss is 1.03152930737\n",
      "step 1480, loss is 1.03152930737\n",
      "step 1500, loss is 1.03152930737\n",
      "step 1520, loss is 1.03152930737\n",
      "step 1540, loss is 1.03152930737\n",
      "step 1560, loss is 1.03152930737\n",
      "step 1580, loss is 1.03152930737\n",
      "step 1600, loss is 1.03152930737\n",
      "step 1620, loss is 1.03152930737\n",
      "step 1640, loss is 1.03152930737\n",
      "step 1660, loss is 1.03152930737\n",
      "step 1680, loss is 1.03152930737\n",
      "step 1700, loss is 1.03152930737\n",
      "step 1720, loss is 1.03152930737\n",
      "step 1740, loss is 1.03152930737\n",
      "step 1760, loss is 1.03152930737\n",
      "step 1780, loss is 1.03152930737\n",
      "step 1800, loss is 1.03152930737\n",
      "step 1820, loss is 1.03152930737\n",
      "step 1840, loss is 1.03152930737\n",
      "step 1860, loss is 1.03152930737\n",
      "step 1880, loss is 1.03152930737\n",
      "step 1900, loss is 1.03152930737\n",
      "step 1920, loss is 1.03152930737\n",
      "step 1940, loss is 1.03152930737\n",
      "step 1960, loss is 1.03152930737\n",
      "step 1980, loss is 1.03152930737\n",
      "step 2000, loss is 1.03152930737\n",
      "step 2020, loss is 1.03152930737\n",
      "step 2040, loss is 1.03152930737\n",
      "step 2060, loss is 1.03152930737\n",
      "step 2080, loss is 1.03152930737\n",
      "step 2100, loss is 1.03152930737\n",
      "step 2120, loss is 1.03152930737\n",
      "step 2140, loss is 1.03152930737\n",
      "step 2160, loss is 1.03152930737\n",
      "step 2180, loss is 1.03152930737\n",
      "step 2200, loss is 1.03152930737\n",
      "step 2220, loss is 1.03152930737\n",
      "step 2240, loss is 1.03152930737\n",
      "step 2260, loss is 1.03152930737\n",
      "step 2280, loss is 1.03152930737\n",
      "step 2300, loss is 1.03152930737\n",
      "step 2320, loss is 1.03152930737\n",
      "step 2340, loss is 1.03152930737\n",
      "step 2360, loss is 1.03152930737\n",
      "step 2380, loss is 1.03152930737\n",
      "step 2400, loss is 1.03152930737\n",
      "step 2420, loss is 1.03152930737\n",
      "step 2440, loss is 1.03152930737\n",
      "step 2460, loss is 1.03152930737\n",
      "step 2480, loss is 1.03152930737\n",
      "step 2500, loss is 1.03152930737\n",
      "step 2520, loss is 1.03152930737\n",
      "step 2540, loss is 1.03152930737\n",
      "step 2560, loss is 1.03152930737\n",
      "step 2580, loss is 1.03152930737\n",
      "step 2600, loss is 1.03152930737\n",
      "step 2620, loss is 1.03152930737\n",
      "step 2640, loss is 1.03152930737\n",
      "step 2660, loss is 1.03152930737\n",
      "step 2680, loss is 1.03152930737\n",
      "step 2700, loss is 1.03152930737\n",
      "step 2720, loss is 1.03152930737\n",
      "step 2740, loss is 1.03152930737\n",
      "step 2760, loss is 1.03152930737\n",
      "step 2780, loss is 1.03152930737\n",
      "step 2800, loss is 1.03152930737\n",
      "step 2820, loss is 1.03152930737\n",
      "step 2840, loss is 1.03152930737\n",
      "step 2860, loss is 1.03152930737\n",
      "step 2880, loss is 1.03152930737\n",
      "step 2900, loss is 1.03152930737\n",
      "step 2920, loss is 1.03152930737\n",
      "step 2940, loss is 1.03152930737\n",
      "step 2960, loss is 1.03152930737\n",
      "step 2980, loss is 1.03152930737\n",
      "final loss: 1.03152930737\n",
      "model W: 2.99704551697, b: 2.01771807671\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    grads = grad(model, training_inputs, training_outputs)\n",
    "    optimizer.apply_gradients(zip(grads, [model.W, model.b]), global_step=tf.train.get_or_create_global_step())\n",
    "    if i % 20 == 0:\n",
    "        print(\"step {}, loss is {}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "print(\"final loss: {}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"model W: {}, b: {}\".format(model.W.numpy(), model.b.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use objects for state during eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device(\"cpu:0\"):\n",
    "    v = tfe.Variable(tf.random_normal([1000, 1000]))\n",
    "    v = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "x = tfe.Variable(10.)\n",
    "checkpoint = tfe.Checkpoint(x = x)\n",
    "x.assign(2.)\n",
    "save_path = checkpoint.save('./ckpt/')\n",
    "x.assign(11.)  # Change the variable after saving.\n",
    "# Restore values from the checkpoint\n",
    "checkpoint.restore(save_path)\n",
    "print(x.numpy())  # => 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=338632, shape=(), dtype=float64, numpy=2.5>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = tfe.metrics.Mean(\"loss\")\n",
    "metrics(0)\n",
    "metrics(5)\n",
    "metrics.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced automatic differentiation topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line_search_step(fn, init_x, rate=1.0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(init_x)\n",
    "        value = fn(init_x)\n",
    "    grad = tape.gradient(value, init_x)\n",
    "    grad_norm = tf.reduce_sum(grad * grad)\n",
    "    init_value = value\n",
    "    while value > init_value - rate * grad_norm:\n",
    "        x = init_x - rate * grad\n",
    "        value = fn(x)\n",
    "        rate /= 2.0\n",
    "    return x, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    return 2 * x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=28, shape=(), dtype=float32, numpy=1.0>,\n",
       " <tf.Tensor: id=30, shape=(), dtype=float32, numpy=5.0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tfe.Variable(3.)\n",
    "line_search_step(fun, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional functions to compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad = tfe.gradients_function(square)\n",
    "gradgrad = tfe.gradients_function(lambda x: grad(x))\n",
    "gradgradgrad = tfe.gradients_function(lambda x: gradgrad(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=68, shape=(), dtype=int32, numpy=2>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradgrad(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradgradgrad(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=91, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=102, shape=(), dtype=float32, numpy=1.0>]\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.log(1 + e), grad\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# As before, the gradient computation works fine at x = 0.\n",
    "print(grad_log1pexp(0.))  # => [0.5]\n",
    "\n",
    "# And the gradient computation also works at x = 100.\n",
    "print(grad_log1pexp(100.))  # => [1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure(x, steps):\n",
    "    tf.matmul(x, x)\n",
    "    start = time.time()\n",
    "    for i in range(steps):\n",
    "        v = tf.matmul(x, x)\n",
    "        _ = v.numpy()\n",
    "    end = time.time()\n",
    "    return (end - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = [1000, 1000]\n",
    "steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu: 5.12560606003s\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    x = tf.random_normal(shape)\n",
    "    period = measure(x, steps)\n",
    "    print(\"cpu: {}s\".format(period))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use eager execution in Graph execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eager_fun(x):\n",
    "    res = tf.matml(x, x)\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    v = tf.placeholder(dtype=tf.float32)\n",
    "    pf = tfe.py_func(eager_fun, [v], tf.float32)\n",
    "#     sess.run(pf, feed_dict={v: [[2.0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
