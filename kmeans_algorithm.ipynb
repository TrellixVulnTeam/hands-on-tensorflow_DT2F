{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(object):\n",
    "    \"\"\" Implement KMeans in tensorflow\n",
    "        with Lloyd Algorithm\n",
    "        1. init: randomly init K cluster centroids\n",
    "        2. cluster assignment: X to its closest centroids\n",
    "        3. move centroid step: compute means of each centroid and do new assignments\n",
    "    \"\"\"\n",
    "    def __init__(self, num_clusters, max_steps=100, random_state=None):\n",
    "        self.num_clusters = num_clusters\n",
    "        self.max_steps = max_steps\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
    "            Training instances to cluster.\n",
    "        \"\"\"\n",
    "        self._train(X)\n",
    "    \n",
    "    def get_cluster_centers(self):\n",
    "        \"\"\" Get cluster centers, please call this after fit\n",
    "        Returns:\n",
    "            cluster_centers: an array of cluster centers\n",
    "        \"\"\"\n",
    "        return self.cluster_centers_\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" do predictions for samples\n",
    "        \n",
    "        Args:\n",
    "            X: ndarray, samples object\n",
    "        Returns:\n",
    "            labels: array, Index of the cluster centroid each sample belongs to\n",
    "        \"\"\"\n",
    "        graph = tf.Graph()\n",
    "        m, n = X.shape\n",
    "        with graph.as_default():\n",
    "            x_tensor = tf.placeholder(tf.float32, shape=X.shape, name=\"X_test\")\n",
    "            centroids = tf.convert_to_tensor(self.cluster_centers_)\n",
    "            predictions = self._find_closest_centroids(x_tensor, centroids)\n",
    "            with tf.Session() as sess:\n",
    "                predictions = sess.run(predictions, feed_dict={x_tensor: X})\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def _init_centroids(self, X, num_centroids):\n",
    "        \"\"\"init cluster centers randomly\n",
    "        \n",
    "        Args:\n",
    "            X: Tensor object, input samples\n",
    "            num_centroids: python integer, the number of centroids\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"init_centroid\"):\n",
    "            k = num_centroids\n",
    "            centroids = tf.random_shuffle(X, seed=self.random_state)[:k,]\n",
    "            print(\"randomly init centroids number: {}\".format(k))\n",
    "        return centroids\n",
    "    \n",
    "    \n",
    "    def _get_distance(self, X, vector):\n",
    "        \"\"\" Calculate euclid distance between samples X and center vector\n",
    "        \n",
    "        Args:\n",
    "            X: Tensor, shape is m * n\n",
    "            vector: Tensor, shape is 1 * n\n",
    "        \n",
    "        Returns:\n",
    "            a tensor object of distances, such as [[0.1], [0.2], [3.4]]\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"get_distance\"):\n",
    "            m, n = X.shape\n",
    "            # reduce_sum output rank is like [a, b, c], reshape it the get vector like [[a], [b], [c]]\n",
    "            # reshape it for concat tensor convenience\n",
    "            distance = tf.sqrt(tf.reduce_sum(tf.square(X - vector), axis=1)) # axis =1 means by column\n",
    "            distance = tf.reshape(distance, [m, 1])\n",
    "        return distance\n",
    "    \n",
    "    \n",
    "    def _find_closest_centroids(self, X, centroids):\n",
    "        \"\"\" assign each sample to the closest centroid\n",
    "        Args:\n",
    "            X: tensor object, the samples, shape is [m, n]\n",
    "            centroids: tensor object, the centroids, shape is [k, n]\n",
    "            \n",
    "        Returns:\n",
    "            idx: tensor object, Index of the cluster centroid of which each sample belongs to\n",
    "                 return object is like [0, 1, 0, 1], shape is [1, m]\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"find_closest_centroids\"):\n",
    "            m, n = X.shape\n",
    "            k, n = centroids.shape\n",
    "            distances = []\n",
    "            for i in range(k):\n",
    "                d = self._get_distance(X, centroids[i])\n",
    "                distances.append(d)\n",
    "            # assign centroids of each row    \n",
    "            # concated by column, distances[:, i] is the distance between X and center i\n",
    "            distances = tf.concat(distances, axis=1)\n",
    "            # get the index of minumum distances for each sample\n",
    "            idx = tf.cast(tf.argmin(distances, axis=1), tf.int32)\n",
    "        return idx\n",
    "    \n",
    "    \n",
    "    def _compute_new_centroids(self, X, idx, num_centroids):\n",
    "        \"\"\" Compute new centroids by computing mean of each cluster group       \n",
    "        Args:\n",
    "            X: tensor object, the samples, shape is [m, n]\n",
    "            idx: tensor object, the index of centroids for each sample, shape is [m]\n",
    "            num_centroids: python integer, the number of centroids\n",
    "        \n",
    "        Returns:\n",
    "            a tensor object of new centroids, format is like [[0.5, 0.5], [1, 2]]\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"compute_new_centroids\"):\n",
    "            m, n = X.shape\n",
    "            k = num_centroids\n",
    "            centroids = []\n",
    "            for i in range(k):\n",
    "                centroid_no = tf.ones(m, dtype=tf.int32) * i\n",
    "                mask = tf.equal(idx, centroid_no) # get bool mask for cluster i\n",
    "                cluster_group = tf.boolean_mask(X, mask)\n",
    "                centroid = tf.reduce_mean(cluster_group, axis=0) # shape is [n]\n",
    "                centroid = tf.reshape(centroid, [1, n]) # reshape for concat convenience\n",
    "                centroids.append(centroid)\n",
    "            centroids = tf.concat(centroids, axis=0)\n",
    "        return centroids\n",
    "        \n",
    "    def _train(self, X):\n",
    "        \"\"\" train data\n",
    "        \n",
    "        Args:\n",
    "            X: python ndarray\n",
    "        \"\"\"\n",
    "        graph = tf.Graph()\n",
    "        k = self.num_clusters\n",
    "        max_steps = self.max_steps\n",
    "        m, n = X.shape\n",
    "        with graph.as_default():\n",
    "            # Build graph\n",
    "            x_tensor = tf.placeholder(tf.float32, shape=[m, n], name=\"X\")\n",
    "#             with tf.variable_scope(\"centroids\", reuse=tf.AUTO_REUSE):\n",
    "#                     centroids_var = tf.get_variable(\"centroids\", shape=[k, n], initializer=tf.ones_initializer())\n",
    "            centroids = self._init_centroids(x_tensor, k)\n",
    "            for i in range(max_steps):\n",
    "                with tf.name_scope(\"train_step_{}\".format(i)):\n",
    "                    idx = self._find_closest_centroids(x_tensor, centroids)\n",
    "                    centroids = self._compute_new_centroids(x_tensor, idx, k)\n",
    "            with tf.Session() as sess:\n",
    "#                 sess.run(tf.global_variables_initializer())\n",
    "                self.labels_, self.cluster_centers_ =  sess.run([idx, centroids], feed_dict={x_tensor: X})\n",
    "                self.labels_tensor_ = tf.convert_to_tensor(self.labels_)\n",
    "                self.cluster_centers_tensor_ = tf.convert_to_tensor(self.cluster_centers_)\n",
    "#                 print(self.labels_)\n",
    "#                 print(self.cluster_centers_)\n",
    "                \n",
    "    def _test(self, X):\n",
    "        \"\"\" train data\n",
    "        \"\"\"\n",
    "        graph = tf.Graph()\n",
    "        X_data = np.array([[1., 0.], [0, 1], [2., 2.], [3., 3.]])\n",
    "        [m, n] = X_data.shape\n",
    "        k = 2\n",
    "        with graph.as_default():\n",
    "            x_tensor = tf.placeholder(tf.float32, shape=[m, n], name=\"X\")\n",
    "            centroids = tf.constant([[1., 1.], [2., 2.]])\n",
    "            idx = tf.constant([0, 0, 1, 1])\n",
    "            d = self._compute_new_centroids(x_tensor, idx, k)\n",
    "        with tf.Session(graph=graph) as sess:\n",
    "#             sess.run(tf.global_variables_initializer())\n",
    "            res = sess.run(d, feed_dict={x_tensor: X_data})\n",
    "            print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomly init centroids number: 2\n",
      "[0 0 0 1 1 1]\n",
      "[[ 1.  2.]\n",
      " [ 4.  2.]]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(num_clusters=2, max_steps=10, random_state=42)\n",
    "X = np.array([\n",
    "    [1, 2], [1, 4], [1, 0],\n",
    "    [4, 2], [4, 4], [4, 0]\n",
    "])\n",
    "kmeans._train(X)\n",
    "print kmeans.labels_\n",
    "print kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[1, 1], [4, 3], [5, 6]])\n",
    "kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>120</th>\n",
       "      <th>4</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   120    4  setosa  versicolor  virginica\n",
       "0  6.4  2.8     5.6         2.2          2\n",
       "1  5.0  2.3     3.3         1.0          1\n",
       "2  4.9  2.5     4.5         1.7          2\n",
       "3  4.9  3.1     1.5         0.1          0\n",
       "4  5.7  3.8     1.7         0.3          0\n",
       "5  4.4  3.2     1.3         0.2          0\n",
       "6  5.4  3.4     1.5         0.4          0\n",
       "7  6.9  3.1     5.1         2.3          2\n",
       "8  6.7  3.1     4.4         1.4          1\n",
       "9  5.1  3.7     1.5         0.4          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test with iris data\n",
    "iris = pd.read_csv(\"./datasets/iris/iris_training.csv\")\n",
    "iris.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomly init centroids number: 3\n",
      "[1 0 0 2 2 2 2 1 0 2 0 1 2 2 1 0 1 1 1 2 1 1 2 1 1 2 0 1 0 1 0 2 0 1 1 1 1\n",
      " 1 2 2 1 1 1 2 2 1 2 1 2 1 2 0 0 2 1 0 1 1 1 0 0 1 1 1 0 1 2 1 1 2 2 0 2 1\n",
      " 1 2 0 0 0 1 2 0 0 0 1 2 1 0 1 2 1 0 2 2 1 2 2 1 0 2 2 0 2 0 2 2 2 2 0 2 1\n",
      " 0 2 1 2 0 0 2 2 0]\n",
      "[[ 5.80937433  2.73750019  4.22812462  1.32499993  1.0625    ]\n",
      " [ 6.67999935  3.00222206  5.5422225   1.99111164  1.88888884]\n",
      " [ 4.99767494  3.3744185   1.48837209  0.2697674   0.02325581]]\n"
     ]
    }
   ],
   "source": [
    "iris_train = iris.as_matrix()[:, :4]\n",
    "kmeans = KMeans(num_clusters=3, max_steps=1, random_state=42)\n",
    "kmeans.fit(iris.as_matrix())\n",
    "print kmeans.labels_\n",
    "print kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 2], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_test = iris.as_matrix()[:4, :]\n",
    "kmeans.predict(iris_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
