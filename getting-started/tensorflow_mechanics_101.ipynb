{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy\n",
    "#from tensorflow.examples.tutorials.mnist import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets(\"./datasets/mnist_data/\", False) # one hot encoding must be false here\n",
    "mnist_data.train.labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "image_classes = 10\n",
    "image_size = 28\n",
    "hidden1_units = 128\n",
    "hidden2_units = 32\n",
    "image_pixels= image_size * image_size\n",
    "images = tf.placeholder(tf.float32, shape=(batch_size, image_pixels))\n",
    "labels = tf.placeholder(tf.int32, shape=(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(128)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_pixels, hidden1_units], stddev = 1.0 / math.sqrt(float(image_pixels))), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden1_units]), name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "hidden1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100), Dimension(32)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden1_units, hidden2_units], stddev=1.0/math.sqrt(float(hidden1_units))), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden2_units]), name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "hidden2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('softmax_linear'):\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([hidden2_units, image_classes], stddev=1.0/math.sqrt(float(hidden2_units))), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([image_classes]), name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "print(logits.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(100)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "cross_entropy.shape\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.scalar('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build evaluate metrix\n",
    "correct_prediction = tf.nn.in_top_k(logits, labels, 1)\n",
    "accuracy = tf.reduce_sum(tf.cast(correct_prediction, tf.float16))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, accuracy: 5.0, loss_value: 2.33468699455\n",
      "add summary for step 0\n",
      "step: 100, accuracy: 84.0, loss_value: 0.56485170126\n",
      "add summary for step 100\n",
      "step: 200, accuracy: 93.0, loss_value: 0.393768996\n",
      "add summary for step 200\n",
      "step: 300, accuracy: 92.0, loss_value: 0.411037325859\n",
      "add summary for step 300\n",
      "step: 400, accuracy: 94.0, loss_value: 0.340746223927\n",
      "add summary for step 400\n",
      "step: 500, accuracy: 90.0, loss_value: 0.373371213675\n",
      "add summary for step 500\n",
      "step: 600, accuracy: 91.0, loss_value: 0.397236406803\n",
      "add summary for step 600\n",
      "step: 700, accuracy: 98.0, loss_value: 0.202075913548\n",
      "add summary for step 700\n",
      "step: 800, accuracy: 95.0, loss_value: 0.183002889156\n",
      "add summary for step 800\n",
      "step: 900, accuracy: 97.0, loss_value: 0.199020683765\n",
      "add summary for step 900\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # build summary saver\n",
    "    saver = tf.train.Saver()\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary_merged = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(\"./logs/mnist/fully_connected\", sess.graph)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for step in xrange(1000):\n",
    "        batch_images, batch_labels = mnist_data.train.next_batch(100, False)\n",
    "        feed_dict = {images: batch_images, labels: batch_labels}\n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            step_accuracy = sess.run(accuracy, feed_dict=feed_dict)\n",
    "            print(\"step: {}, accuracy: {}, loss_value: {}\".format(step, step_accuracy, loss_value))\n",
    "            # Update the events file.\n",
    "            summary_str = sess.run(summary_merged, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            print(\"add summary for step {}\".format(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
